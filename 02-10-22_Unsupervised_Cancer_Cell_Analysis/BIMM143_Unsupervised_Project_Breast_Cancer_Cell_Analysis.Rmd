---
title: "Unsupervised Learning Analysis of Human Breast Cancer Cells"
author: "Joshua Cheung"
date: "02/08/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Exploratory data analysis

## Preparing the data.

We first use the read.csv() function to reach the CSV file containing the data.  

```{r}
# Here we save the input data file into our Project directory.
fna.data <- "WisconsinCancer.csv"
# We assign the result of the above code to an object called wisc.df.
wisc.df <- read.csv(fna.data, row.names=1)
```

We now examine the input data to ensure column names are set correctly.  We use the head() function to preview the first 6 rows.

```{r}
head(wisc.df)
```

We now note that the first column here (wisc.df$diagnosis) is a pathologist's expert diagnosis.  We wil not be using this for the unsupervised analysis in this project.  Thus, to ensure we don't accidently include this column we define a new data.frame which omits this first column.

```{r}
# Note we use -1 index here to remove the first column.
wisc.data <- wisc.df[,-1]
```

Now, we also define a separate vector called diagnosis that contains the data from the diagnosis the column of the original data set.

```{r}
# We create diagnosis vector for later.
diagnosis <- as.factor(wisc.df$diagnosis)
```

# Exploratory data analysis

> **Q1. How many observations are in this dataset?**

```{r}
nrow(wisc.data)
```
There are 569 observations in this data set.

> **Q2. How many of the observations have a malignant diagnosis?**

```{r}
length(grep("M", diagnosis))
```
There are 212 observations with a malignant diagnosis.

> **Q3. How many variables/features in the data are suffixed with** "_mean"**?**

```{r}
length(grep("_mean", colnames(wisc.data)))
```

There are 10 variables/features in the data suffixed with _mean.

# Principal Component Analysis

## Performing PCA

We first check the standard deviation of the features of the wisc.data to determine if the data should be scaled.

```{r}
# We check the column means and standard deviations.
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

We then execute the PCA with the prcomp() function on the wisc.data and scale if appropriate.  We also assign the output model to wisc.pr.

```{r}
# We perform PCA on the wisc.data.
wisc.pr <- prcomp(wisc.data, scale=TRUE)
```

Now we look at a summary of the results of wisc.pr.

```{r}
# We inspect the summary.
summary(wisc.pr)
```

> **Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?**

The summary print out indicate PC1 account for 44.3% of the original variance.

> **Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?**

Three principal components (PC1, PC2, and PC3) are required to describe at least 70% of the original variance in the data.

> **Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?**

Seven principal components (PC1, PC2, PC3, PC4, PC5, PC6, and PC7) are required to describe at least 90% of the original variance in the data.

## Interpreting PCA results

We create a biplot of the wisc.pr using the biplot() function.

```{r}
biplot(wisc.pr)
```

> **Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?**

We see that there is not a central group of variable around the the middle of each principal component.  Instead we see that the variable are clustered all around the periphery, and there are not discernible groups.  As a results this plot is difficult to understand.

We generate a more standard scatter plot of each observation along PC1 and PC2 and color the points by the diagnosis.

```{r}
# We create scatter plot observations by components 1 and 2.
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

> **Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?**

```{r}
# We create scatter plot observations by components 1 and 3.
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis , 
     xlab = "PC1", ylab = "PC3")
```

To answer question 8, we see that PC2 accounts for more of the variance in the original data than principal component 3.  As a result, the plot of PC1 vs PC2 has greater separation of the malignant (red) and benign (black) subgroups than the plot of PC1 and PC3.  Also, both plots generally indicate the PC1 is capturing a separation of malignant and benign samples.

Now we turn to using ggplot2 to make the figure more attractive.  Recall that ggplot2 will require a data.frame input.  Additionally, we will also require our diagnosis vector as a column if we want to use it for mapping to the plot color aesthetic.

```{r}
# We first create a data.frame for ggplot2.
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
# Be sure to load the ggplot2 package.
library(ggplot2)
# Now we make a scatter plot colored by the diagnosis vector.
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

## Variance explained

We calculate the variance of each principal component by squaring the sdev component of wisc.pr and saving the result to an object called pr.var.

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
# We preview the first six results of pr.vr
head(pr.var)
```

Now we compute the variance explained by each principal component by dividing the total variance explained of all principal components.  We assign this to a variable called pve and creat a plot of variance explained for each principal component.

```{r}
# We define pve as the variance explained by each principal component.
pve <- pr.var/sum(pr.var)
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

We now consider an alternative scree plot of the same data.  We create a bar plot as follows.

```{r}
# Note the data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

We now note that the CRAN package factoextra is helpful for PCA,  We use this package as follows:

```{r}
# ggplot based graph
# install.packages("factoextra") ## Un-comment to install if necessary
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

# Communicating PCA results

> **Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?**

```{r}
wisc.pr$rotation[,1]
```

Answering question 9, we see that the component of the loading vector for the feature concave.points_mean is -0.26.  This value is relatively small compared to the other feature's values.  This indicates that this variable's influence upon the principal components is slightly smaller relative to the other variables' contributions.

> **Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?**

The minimum number of principal component required to explain 80% of the variance of the data is five principle components (PC1, PC2, PC3, PC4, and PC5).

# 3. Hierarchial clustering

We first scale the wisc.data and assign the result to data.scaled.

```{r}
# We scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Now, we compute the Euclidean distances between all pairs of obeservations in the new scaled dataset and assign this result to data.dist.

```{r}
data.dist <- dist(data.scaled)
```

Now we can create the hierarchical clustering model using complete linkage.  We manually specify the argument method to hclust() and assign the result to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

## Results of hierarchial clustering

> **Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?**

```{r}
# Use the plot() function.
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
To answer question 11, the clustering model has 4 clusters at a height of 19.

## Selecting number of clusters

We use cutree() to cut the tree so that it has 4 clusters.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

We can now use the table() function to compare the cluster membership to the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Thus we have picked 4 clusters.  We see that cluster 1 largely corresponds to malignant cells (with a diagnosis value of 1), and cluster 3 largely corresponds to benign cells (with diagnosis values of 0).

We now compare the results we just got, with a the results from different number of clusters.

> **Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?**

```{r}
# For k=2 clusters.
wisc.hclust.clusters2 <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters2, diagnosis)
# For k=3 clusters.
wisc.hclust.clusters3 <- cutree(wisc.hclust, k=3)
table(wisc.hclust.clusters3, diagnosis)
# For k=5 clusters.
wisc.hclust.clusters5 <- cutree(wisc.hclust, k=5)
table(wisc.hclust.clusters5, diagnosis)
# For k=6 clusters.
wisc.hclust.clusters6 <- cutree(wisc.hclust, k=6)
table(wisc.hclust.clusters6, diagnosis)
# For k=7 clusters.
wisc.hclust.clusters7 <- cutree(wisc.hclust, k=7)
table(wisc.hclust.clusters7, diagnosis)
# For k=8 clusters.
wisc.hclust.clusters8 <- cutree(wisc.hclust, k=8)
table(wisc.hclust.clusters8, diagnosis)
# For k=9 clusters.
wisc.hclust.clusters9 <- cutree(wisc.hclust, k=9)
table(wisc.hclust.clusters9, diagnosis)
# For k=10 clusters.
wisc.hclust.clusters10 <- cutree(wisc.hclust, k=10)
table(wisc.hclust.clusters10, diagnosis)
```

To answer the question 12, after checking all cluster vs diagnoses matches for different numbers of cluster between 2 and 10, we were unable to find a better cluster vs diagnoses match than the one generated for 4 clusters.

## Using different methods

> **Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.**

```{r}
# We try the single linkage method:
wisc.hclust.single <- hclust(data.dist, method="single")
plot(wisc.hclust.single)
```

```{r}
# We try the average linkage method:
wisc.hclust.average <- hclust(data.dist, method="average")
plot(wisc.hclust.average)
```

```{r}
# We try the ward.D2 linkage method:
wisc.hclust.wardd2 <- hclust(data.dist, method="ward.D2")
plot(wisc.hclust.wardd2)
```

Answering question 13, our favorite method to use is the ward.D2 method as this yields the dendogram that appears to be the neatest and most easy to view out of all the methods.  There are two clear main branches of the dendogram indicating two main clusters.  The dendogram from the single linkage method is rather messy and difficult to view, and the dendogram from the average linakge method is slightly cleaner, but very similar to the complete linkage method's dendogram.

# 4. OPTIONAL: K-means clustering

We create a k-means model on wisc.data and assign the result to wisc.km.

```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 20)
```

We use the table function to compare the cluster membership of the k-means model to the actual diagnoses contained in the diagnosis vector.

```{r}
table(diagnosis, wisc.km$cluster)
```

We also compare the k-means model to the hierarchical clustering model.

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```

> **Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?**

K-means separates the two diagnoses fairly well as we see a clear distinction between the two groups.  When comparing it with the hclust, we see that clusters 1, 2, and 4 from the hclust results of the second table are roughly equivalent to cluster 1 of the k-means results from the first table.  Similarly,clusters 3 from the hclust results of the second table are roughly equivalent to cluster 2 of the k-means results from the first table.

# 5. Combining methods

## Clustering on PCA results

We create a hierarchical clustering model with the linkage method="ward.D2" and assign the results to wisc.pr.hclust.

```{r}
wisc.pr.hclust <- hclust(data.dist, method="ward.D2")
plot(wisc.hclust.wardd2)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)

```

Note that there is color swap here.  We fix this as follows.

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Re-plot using our re-ordered factor.
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
# We use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7].
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
```

```{r}
# We now cut the model into 2 clusters:
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
# Now we compare the results to actual diagnoses.
table(wisc.pr.hclust.clusters, diagnosis)
```

> **Q15. How well does the newly created model with four clusters separate out the two diagnoses?**

The newly created model sorts out the two diagnoses relatively well,  We can clearly see that cluster one is mostly "M" and cluster 2 is mostly "B".

> **Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.**

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

Answering question 16, k-means separates the diagnoses relatively well.  We see that for k-means, cluster one is clearly mostly "B" and cluster 2 is mostly "M".  We see that in the hierarchical cluster table, clusters 1, 2, and 4 are equivalent to cluster 2 in the k-means table.  Additionally cluster 3 in the appear of the hierarchical cluster results appear to be equivalent.

# 6. Sensitivty/Specificty

> **Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?**

We analyze the specificity and sensitivity of the k-means method.

```{r}
km.table <- table(wisc.km$cluster, diagnosis)
# We see that cluster that is predominantly malignant is cluster 2.
# So the sensitivity is:
round(km.table[1, "M"]/(km.table[1, "M"] + km.table[2, "M"]), digits=3)
# We see that cluster that is predominantly benign is cluster 1.
# So the specificity is:
round(km.table[2, "B"]/(km.table[1, "B"] + km.table[2, "B"]), digits =3)
```
We analyze the specificity and sensitivity of the hierarchical clustering method with complete linkages.

```{r}
hclust.table <- table(wisc.hclust.clusters, diagnosis)
# We see that cluster that is predominantly malignant is cluster 1.
# So the sensitivity is:
round(hclust.table[1, "M"]/(hclust.table[1, "M"] + hclust.table[2, "M"] + 
                          hclust.table[3, "M"] + hclust.table[4, "M"]), 
      digits=3)
# We see that cluster that is predominantly benign is cluster 3.
# So the specificity is:
round(hclust.table[3, "B"]/(hclust.table[1, "B"] + hclust.table[2, "B"] + 
                          hclust.table[3, "B"] + hclust.table[4, "B"]), 
      digits=3)
```

We analyze the specificity and sensitivity of the hierarchical clustering method with ward.d2 linkages.

```{r}
hclust2.table <- table(grps, diagnosis)
# We see that cluster that is predominantly malignant is cluster 1.
# So the sensitivity is:
round(hclust2.table[1, "M"]/(hclust2.table[1, "M"] + hclust2.table[2, "M"]),
      digits=3)
# We see that cluster that is predominantly benign is cluster 2.
# So the specificity is:
round(hclust2.table[2, "B"]/(hclust2.table[1, "B"] + hclust2.table[2, "B"]), 
      digits=3)

```

To answer question 17, we see that the both the Hierarchical clustering with complete linkage and and k-means models have equal highest specificity which is slightly higher than those of the model for Hierarchical clustering with ward.d2 linkage.  Additionally, the k-means model, has the greatest sensitivity, followed by the model of Hierarchical clustering with complete linkage, followed lastly by the model for Hierarchical clustering with ward.d2 linkage.

# 7. Prediction
We will now use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
# url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

Now we plot this information.

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> **Q18. Which of these new patients should we prioritize for follow up based on your results?**

We should prioritize the second patient for followup as their samples fall well within the red cluster which contain the more malignant cancerous samples.
